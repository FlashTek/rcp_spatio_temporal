\chapter{Überblick über Recurrent Neural Networks}
In den letzten Jahren hat die Technik der Neuronalen Netzwerke erneut stark an Popularität gewonnen. Dies liegt zum einen an der gestiegenen verfügbaren Rechenleistung und zum anderen an der Entwicklung hierfür notwendiger Algorithmen.\\
Allgemein lassen sich diese Netze in zwei große Gruppen aufteilen: die der \textsc{Feed Forward Neural Networks} und die der \textsc{Recurrent Neural Networks}, welche im Folgenden als \textsc{FFNN} respektive \textsc{RNN} bezeichnet werden.
Ein \textsc{FFNN} besteht aus mehreren Ebenen, welche jeweils aus verschiedenen nicht-linearen Einheiten zusammengesetzt sind. Hierbei wird die erste Ebene für die Eingabe eines Signals und die letzte Ebene als Ausgabe genutzt. Die Einheiten zweier benachbarter Ebenen sind mit individuellen Gewichten vollständig in Richtung der Ausgabe verbunden. Dies bedeutet, dass jedes Einheit $x^n_i$ sein Signal an alle Einheiten der folgenden Ebene $x^{n+1}_j$ mit einem individuellen Gewicht $w^n_{i \rightarrow j}$ weitergibt. Zwischen den Einheiten innerhalb einer Ebene bestehen keinerlei Verbindungen.
Damit ein solches Netzwerk Vorhersagen treffen kann, muss es trainiert werden. Dies wurde durch die Entwicklung des \textsc{Backpropagation}-Algorithmus stark vereinfacht.\\

\textsc{RNN} haben einen ähnlichen Aufbau, doch sind bei Ihnen nicht nur Informationsweitergaben in Richtung der Ausgabeebene möglich, sondern auch in die entgegengesetzte Richtung - es können somit alle Einheiten an alle anderen Einheiten Signale weitergeben und von diesen erhalten. Dies kann die Vorhersage in bestimmten Anwendungsbeispielen wie der Text und Sprachanalyse verbessern. Mit diesem Vorteil kommt allerdings auch der Nachteil, dass zum Trainieren nicht mehr der einfachere \textsc{Backpropagation}-Algorithmus Algorithmus genutzt werden kann, sondern eine für \textsc{RNN}s abgewandelte Form genutzt werden muss. Hierfür werden die verschiedenen Zustände die das \textsc{RNN} im Laufe der Signal-Propagation annimmt nacheinander betrachtet und auf diese zeitliche Entwicklung anschließend der \textsc{Backpropagation}-Algorithmus angewendet. Diese Methode ist unter dem Namen \textsc{Backpropagation through Time} (BTT) bekannt. Sie ist rechenaufwendiger und dadurch, dass das Verschwindenden des Gradienten wahrscheinlicher ist, instabiler.\\

Um dieses Problem zu lösen wurden unter anderem die \textsc{Echo State Networks} (ESN) entwickelt.

