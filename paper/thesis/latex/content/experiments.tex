\chapter{Anwendungen}
\label{ch:experiments}
Die zuvor in Kapitel \ref{ch:theory} eingeführten Methoden werden nun in drei verschiedene Szenarien ausprobiert und verglichen. Der Fokus liegt dabei auf der Verwendung und Erprobung der \textsc{ESN}s. Da die klassischen Methoden der \textit{Nächsten-Nachbarn} (\textsc{NN}) und der \textit{radialen Basisfunktionen} (\textsc{RBF}) bereits seit längerer Zeit bekannt sind und eine populäre Lösung solcher Problemfälle darstellen, dienen sie als Bezugsgröße.\\

Jedes der drei Szenarien wird sowohl auf ein \textit{Barkley}-System als auch auf ein System nach dem \textit{Mitchell-Schaeffer}-Modell angewendet. Diese Systeme bestehen aus $150 \times 150$ Gitterpunkten und nutzen die zuvor beschriebenen Parameter. Für ihre Startverteilung werden die Felder der beiden Systemvariablen in $10 \times 10 = 100$ gleichgroße Quadrate mit einer Seitenlänge von $15$ Gitterpunkten unterteilt. Innerhalb dieser Quadrate werden die Variablen homogen mit einem Zufallswert zwischen $0$ und $1$ initialisiert. Anschließend werden die Systeme über $20000$ Zeitschritte ($\widehat{=} 200.0$ Zeiteinheiten) simuliert, um ein transientes Verhalten abzuwarten. Durch das weitere Simulieren der Systeme wird der wirkliche Datensatz generiert, der benutzt wird. Dieser wird in einen Trainingsdatensatz, einen Evaluationsdatensatz und einen Testdatensatz aufgeteilt. Der erste Datensatz wird für das Trainieren der Ansätze verwendet, der zweite für das Auswählen der optimalen Hyperparameter und der dritte für die finale Bewertung der Leistung eines Ansatzes. Für das \textit{Barkley}-System wird eine Samplingzeit von $0.1$, für das \textit{Mitchell-Schaeffer}-Modell von $1.0$ und für das \textit{BOCF}-Modell von $2.0$ Zeiteinheiten benutzt.\\
 
Die erste Aufgabe besteht darin, aus der Kenntnis einer der beiden Systemvariablen die andere Unbekannte zu ermitteln (Kreuzvorhersage). Dabei wird die Spannungsvariable als Quelle genutzt. Dies ist in den zuvor eingeführten Modellen jeweils die Größe, welche den Diffusionsterm beinhaltet; also die $u$-Variable im \textit{Barkley}-Modell und die $v$-Variable im \textit{Mitchell-Schaeffer}-Modell.\\
Im zweiten Szenario werden die Techniken verwendet, um aus Messdaten einer simulierten Fernfeldmessung der Spannungsvariable  diese wiederherzustellen. Diese Fernfeldmessung wird durch eine gauß'sche Unschärfe simuliert.\\
Abschließend wird die Spannungsvariable der inneren Punkte eines Quadrates nur durch die Kenntnis der Randwerte des Systems vorhergesagt.\\

Für das \textit{BOCF}-Modell wird, um den Umfang dieser Arbeit beschränkt zu halten, lediglich die erste Aufgabe betrachtet: Es wird eine Kreuzvorhersage zwischen der $u(t)$ Variable und der $v(t)$, $w(t)$ und $s(t)$ Variable durchgeführt. Dazu wird zuerst ein \textit{BOCF}-System mit $500 \times 500$ Gitterpunkten und den zuvor eingeführten Konstanten simuliert. Hierbei wird zuerst wieder ein transientes Verhalten abgewartet, um anschließend mit einer Samplingzeit von $2.0$ Zeiteinheiten die Zeitreihe aufzuzeichnen. Die gewählte Systemgröße wird benötigt, um eine langanhaltende chaotische Dynamik zu erzeugen. Im Anschluss werden die Felder des Systems auf $150 \times 150$ Einheiten mittels einer \textit{bilinearen Interpolation} reskaliert, um die gleiche Anzahl an Gitterpunkten wie bei den anderen beiden Modelle zu betrachten. 

\unsure{Add subchapter?}
\section{Allgemeines Vorgehen}
\label{sc:experiments_general}
Das Ziel aller drei Aufgaben besteht jeweils darin ein zweidimensionales Feld vorherzusagen. Eine naheliegende Möglichkeit dies zu erreichen besteht darin, den gesamten Inhalt des $150 \times 150$ Einheiten großen Feldes auf einmal vorherzusagen. Da dabei die Ausgabe der Vorhersage aus einem $22500$-dimensionalen Vektor besteht, werden sehr viele Trainingsdaten benötigt, um genügend Informationen über eine solch hochdimensionale Ausgabe zu erhalten. Um dieses Problem zu umgehen, wird stattdessen ein Verfahren benutzt, bei dem jeder Punkt einzeln vorhergesagt wird. Dieses Aufteilen einer großen Vorhersage in viele kleinere bietet zudem eine Verbesserung der Ressourcennutzung, bei der zugleich der Bedarf an Arbeitsspeicher und auch die Rechenzeit sinkt (Details siehe \ref{sec:exp_general_esn}). Eine schematische Darstellung ist in Abbildung \ref{fig:apx_general_prediction_flowchart} zu finden.\\

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/illustrations/sigma_patches.pdf}
  \caption{Messsonde ohne Abstände zwischen\\den Messpunkten}
  \label{fig:probe_illustration_no_gaps}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{figures/illustrations/sigma_patches_gaps.pdf}
  \caption{Messonde mit einem Abstand von zwei Einheiten zwischen den Messpunkten}
  \label{fig:probe_illustration_gaps}
\end{subfigure}
\caption{Illustration der verwendeten \textit{Messsondentechnik}. Abbildung \ref{fig:probe_illustration_no_gaps} deutet an, wie aus einem $\sigma^2$ großem Quadrat um den eigentlichen Messpunkt Daten für die Vorhersage genutzt werden. Dagegen ist in Abbildung \ref{fig:probe_illustration_gaps} das Verfahren für $\sigma=5$ und $\Delta \sigma = 2$ dargestellt, sodass insgesamt die Information aus $9$ Punkten genutzt wird.}
\label{fig:probe_illustration}
\end{figure}

Des Weiteren kann angenommen werden, dass die Dynamiken einen ausgeprägten lokalen Charakter besitzen, sodass zumindest bei den ersten beiden Aufgaben weit entfernte Punkte keinen unmittelbaren Einfluss auf die Vorhersage haben. Darauf basierend kann eine sogenannte \textit{Messsondentechnik} entwickelt und für diese genutzt werden. Die Idee hierfür ist in \citep{parlitz2000prediction} unter dem Namen \textit{local states} gegeben. Hierbei werden nicht nur die Informationen an einem Punkt $(i, j)$ für die Vorhersage, sondern auch die benachbarten Punkte, welche in einem Quadrat um $(i, j)$ liegen, verwendet. Eine Veranschaulichung ist in Abbildung \ref{fig:probe_illustration_no_gaps} zu finden. Die Größe des Quadrates wird durch den Parameter $\sigma$ bestimmt und ergibt sich zu $\sigma^2$. Da direkte Nachbarn unter Umständen durch den geringen Abstand sehr ähnliche Informationen beinhalten können, wird zudem ein Parameter $\Delta \sigma$ eingeführt, welcher den Abstand zweier benachbarter Punkte, deren Informationen simultan verwendet werden, angibt. Eine beispielhafte Darstellung hiervon ist für $\sigma = 5, \Delta \sigma=2$ in Abbildung \ref{fig:probe_illustration_gaps} dargestellt. Dabei werden nur die Zeitreihen der Gitterpunkte genutzt, welche dunkelgrau hinterlegt sind, und die hellgrauen Informationen verworfen. Die Parameter, welche für die ersten beiden Aufgaben überprüft werden, sind in Tabelle \ref{tab:probe_sigma_values} aufgelistet. Da in der Nähe der Gitterränder nicht genug Punkte zur Verfügung stehen, um die Technik mit $\sigma>1$ zu benutzen, wird $\sigma$ auf dem Weg zu den Rändern schrittweise auf $1$ reduziert.\\
Durch dieses Vorgehen kann für jeden Gitterpunkt ein ${\left \lceil{\frac{\sigma}{\Delta \sigma}}\right \rceil}^2$-dimensionaler Eingabevektor erstellt und für die ersten beiden Vorhersage-Aufgaben genutzt werden. Dabei steht $\lceil \cdot \rceil$ für die Aufrundungsfunktion.\\

\begin{table}[h]
\centering
\begin{tabular}{cc|c|c|c|c|c|c}
\hline
$\sigma$ & 1 & 3 & \multicolumn{2}{c|}{5} & \multicolumn{3}{c}{7} \\
\hline
$\Delta \sigma$ & 1 & 1 & 1 & 2 & 1 & 2 & 3 \\
\hline
\end{tabular} 
\caption{In den ersten beiden Aufgaben verwendete Parameter $\sigma$ und $\Delta \sigma$ für die \textit{Messsondentechnik}.}
\label{tab:probe_sigma_values}
\end{table} 

Der Trainingsvorgang wird jeweils über $N_{Training}=15000$ Schritte durchgeführt und der anschließende Evaluationsdurchgang auf $N_{Evaluation} = 2000$ Schritten. Der finale Testvorgang ist ebenfalls $N_{Testing}=2000$ Schritte lang.
Zur Bewertung der Leistung einer Vorhersage werden die beiden Fehlergrößen MSE und NRMSE eingeführt. Im Allgemeinen ist der MSE (\textit{Mean Squared Error}) durch
\begin{align}
MSE(y) = \frac{1}{m \cdot N_{Testing}} \sum_i^m \sum_t^{N_{Testing}} \left(y(t)_i - \hat{y}(t)_i \right)^2
\end{align}
definiert und charakterisiert die Genauigkeit einer Vorhersage $\hat{y}$ im Vergleich zu dem tatsächlichen Wert $y \in \mathbb{R}^m$ über den Zeitraum $N_{Testing}$. Der NRMSE (\textit{Normalized Root Mean Squared Error}) normiert diesen Fehler noch auf eine Vorhersage, bei der der Mittelwert $\langle y \rangle$ über die Trainingsphase als vorhergesagter Wert genutzt wird. Er ist als
\begin{align}
NRMSE(y) = \sqrt{\frac{MSE(y)}{MSE\left(\langle y \rangle\right)}}
\end{align}
definiert. Ein NRMSE von $0.0$ steht für eine optimale Vorhersage. Steigt der NRMSE auf $>1.0$ an, so ist die Vorhersage durch den Mittelwert des Trainingsdatensatzes präziser als die zuvor bestimmte Vorhersage.\\
Zusätzlich zu diesen Fehlermaßen werden im Folgenden oftmals auch die Laufzeiten der Ansätze angegeben, um ihre Geschwindigkeit einzuordnen. Hierbei ist zu beachten, dass diese nicht über mehrere Ausführungen des identischen Programmes gemittelt worden sind, und deshalb nicht als statistisch signifikante Information sondern nur als ein Hinweis gesehen werden können. Die Laufzeit umfasst dabei sowohl die Trainingszeit, als auch die Zeit die benötigt wird, um eine Vorhersage zu treffen.\\

In Kenntnis, dass in den \textit{Barkley}- und \textit{Mitschell-Schaeffer}-Modellen nur Werte zwischen $0$ und $1$ angenommen werden können, werden die Vorhersagen auf das Intervall $[0, 1]$ beschränkt. Dafür werden die Werte beider Variablen der Systeme sowohl nach unten als auch nach oben hin durch 
\begin{align}
x = \begin{cases}
	0,& \text{wenn } x \leq 0\\
	x,& \text{wenn } 0 \geq x \geq1\\
    1,& \text{wenn } x \geq 1
\end{cases}
\end{align}
abgeschnitten, wobei $x$ für eine der beiden Variablen in dem jeweiligen Modell steht.\\

Zur Erprobung unterschiedlicher Hyperparameter innerhalb der verschiedenen Ansätze ist ein \textsc{Sun Grid Engine}-Cluster genutzt worden. Dabei besteht jeder Knoten aus zwei \textsc{Intel Xeon CPU E5-2650 v2} CPUs mit je $8$ Kernen und einem Takt von $2.650$~GHz und $64$~GB Arbeitsspeicher. Diese werden durch das Betriebssystem \textsc{SUSE Linux Enterprise Server 11} betrieben. Die angegebenen Laufzeiten beziehen sich jeweils auf die Ausführung auf einem dieser Knoten. 
\improvement{Add moreinformation about the cluster}


\FloatBarrier
\subsection{Echo State Network}
\label{sec:exp_general_esn}
\textit{Echo State Networks} besitzen viele verschiedene Hyperparameter, die die Qualität der Vorhersage beeinflussen können. Dazu zählen nach Abschnitt \ref{sc:esn} die Reservoirgröße $N$, der Spektralradius $\rho$, die Verlustrate $\alpha$, die Amplitude der zufälligen Störung $\nu$, die Stärke der Regularisierung $\lambda$ und der Anteil der vorhandenen internen Verbindungen $\epsilon$. Da es zum aktuellen Zeitpunkt noch keinen zufriendenstellenden mathematischen Algorithmus für das selbstständige optimale Einstellen eines \textsc{ESN}s gibt, müssen die Parameter manuell ermittelt werden. Hierfür wird in dieser Arbeit eine \textsc{GridSearch} benutzt. Bei diesem Verfahren wird der Hyperparameterraum in festgelegten Schritten abgetastet und die Leistung des somit entstehenden Netzwerkes evaluiert und dadurch die besten Parameter ermittelt. Wegen der hohen Anzahl von einstellbaren Hyperparametern und der nicht zu vernachlässigenden Rechenzeit für das Trainieren und Evaluieren eines Netzwerkes, ist es nicht sinnvoll, diese Suche für alle Komponenten des hochdimensionalen Zielvektors gleichzeitig durchzuführen. Stattdessen wird zuerst, unter der Annahme, dass die Dynamik sich lokal an allen Punkten ähnlich verhält, ein Punkt in der Mitte des Feldes ausgewählt, und nur versucht diesen einen einzelnen Punkt vorherzusagen. Diese Aufgabe kann deutlich schneller berechnet werden, sodass nun die optimalen Hyperparameter mit einer \textsc{GridSearch} gesucht werden können. Im Anschluss können die Hyperparameter des  zuvor ermittelten \textsc{ESN} für die Vorhersage aller Punkte genutzt werden. Abschließend wird noch einmal versucht, das gefundene Reservoir manuell zu verbessern, indem die Parameter $N$ und $\lambda$ erneut variiert werden.\\
Es ist zu erwarten, dass die hierbei gefundenen Hyperparameter eine akzeptable Leistung für die jeweiligen Probleme erzielen können. Bei dem zuvor beschriebenen Verfahren können bei weitem nicht alle sinnvollen Hyperparameter getestet werden. Stattdessen besteht die Möglichkeit, dass es noch besser geeignete Reservoirs mit anderen Hyperparametern gibt, welche einen noch geringeren Fehler erzielen können.\\

Statt für jeden Punkt einzeln eine Vorhersage zu treffen, ist es bei einem Reservoir-Ansatz auch vorstellbar alle Punkte gleichzeitig vorherzusagen und dabei innerhalb des Reservoirs die räumliche Struktur der Dynamik stärker abzubilden. Hierfür wäre ein sehr großes Reservoir, welches etwa so viele Einheiten besitzt, wie es Gitterpunkte gibt, vorstellbar. Da nach Anhang \ref{sc:apx_runtime_complexity} die Laufzeit des \textsc{ESN} allerdings mit mindestens $\mathcal{O}(N^{3})$ anwächst, würde die Laufzeit mit $(N_{Gitter}^2)^{3}$ zunehmen. Dahingegen wächst die Laufzeit mit dem zuvor vorgestellten Ansatz mit $(N_{Gitter})^2 \cdot N^{3}$, wobei $N$ die Anzahl der internen Einheiten ist. Es ist zu erkennen, dass für hinreichend große Gittergrößen die Laufzeit des großen Ansatzes stärker anwächst als das erläuterte Vorgehen. Zudem lässt sich das zuvor vorgestellte Verfahren auch besser parallelisieren, da die Vorhersage jedes einzelnen Punktes unabhängig von der Vorhersage der übrigen Punkte ist.   
\improvement{Add plot to show the developement of the curves?}

\FloatBarrier
\subsection{Klassische Methoden}
\label{sec:experiments_general_classical}
Die klassischen Methoden sind nicht von alleine in der Lage zeitlich ausgeprägte Dynamiken vorherzusagen, da den Methoden a priori keine Informationen über die vorherigen Zustände vorliegen. Um dieses Problem zu lösen, können \textit{Verzögerungskoordinaten} nach der in Abschnitt \ref{sc:delay_reconstruction} beschriebenen Methode für die in Abschnitt \ref{sc:experiments_general} beschriebenen Vektoren aufgestellt werden. Die über die Autokorrelation ermittelte zeitliche Verzögerung $\tau$ ist für beide Systeme in Tabelle \ref{tab:delay_reconstruction_tau} dargestellt.     

\begin{table}[h]
\centering
\begin{tabular}{cc}
\hline
$\tau_{Barkley}$ & $\tau_{Mitchell-Schaeffer}$ \\ 
0.64 Zeiteinheiten & 2.38 Zeiteinheiten\\ 
\hline 
\end{tabular} 
\caption{Verwendete zeitliche Verzögerung $\tau$ für die \textit{Delay Reconstruction} für das \textit{Mitchell-Schaeffer}- und das \textit{Barkley}-Modell}
\label{tab:delay_reconstruction_tau}
\end{table} 

Durch die Verwendung der Verzögerungskoordinaten vervielfacht sich die Dimensionalität des Eingangssignals. Somit ist das Eingangssignal $\left \lceil \frac{\sigma}{\Delta\sigma} \right \rceil^2 \cdot \delta$-dimensional. 

\input{content/cross_pred.tex}
\input{content/unblur_pred.tex}
\input{content/inner_cross_pred.tex}